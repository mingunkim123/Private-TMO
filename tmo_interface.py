import ollama
import time
import sys
import os
from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
# [ì¶”ê°€] ë°©ê¸ˆ ë§Œë“  ë§¤ë‹ˆì € ë¶ˆëŸ¬ì˜¤ê¸°
from lora_manager import LoRAManager

# 1. ë§¤ë‹ˆì € & ë³´ì•ˆê´€ ì´ˆê¸°í™”
manager = LoRAManager()
print("ğŸ›¡ï¸ ë³´ì•ˆê´€ ëª¨ë¸ ë¡œë”© ì¤‘... (ì²˜ìŒì—” ë‹¤ìš´ë¡œë“œí•˜ëŠë¼ ì‹œê°„ ì¢€ ê±¸ë¦¼)")

# [í•µì‹¬] cache_dirì„ /dataë¡œ ì§€ì •í•´ì•¼ ì¬ë¶€íŒ…í•´ë„ ëª¨ë¸ì´ ì•ˆ ë‚ ì•„ê°!
try:
    print("DEBUG: Starting model load...", flush=True)
    model_name = "dslim/bert-base-NER"
    cache_dir = '/data/models'
    
    print(f"DEBUG: Loading tokenizer for {model_name}...", flush=True)
    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
    print("DEBUG: Tokenizer loaded.", flush=True)

    print(f"DEBUG: Loading model for {model_name}...", flush=True)
    model = AutoModelForTokenClassification.from_pretrained(model_name, cache_dir=cache_dir)
    print("DEBUG: Model loaded.", flush=True)
    
    print("DEBUG: Creating pipeline...", flush=True)
    security_classifier = pipeline(
        "token-classification", 
        model=model, 
        tokenizer=tokenizer,
        device='cpu' # ì ¯ìŠ¨ GPU ë©”ëª¨ë¦¬ ì•„ë¼ë ¤ë©´ CPU ì¶”ì²œ (ì‘ì€ ëª¨ë¸ì´ë¼ CPUë„ ë¹ ë¦„)
    )
    print("âœ… ë³´ì•ˆê´€ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!", flush=True)
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}", flush=True)
    raise

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ê°•ì œ ì¶”ê°€ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ ê¸°ì¤€)
# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜: /data/my_tmo_project/tmo_interface.py
# ë¼ì´ë¸ŒëŸ¬ë¦¬ ìœ„ì¹˜: /data/my_tmo_project/libs
current_dir = os.path.dirname(os.path.abspath(__file__))
libs_path = os.path.join(current_dir, 'libs')
if libs_path not in sys.path:
    sys.path.append(libs_path)

try:
    from groq import Groq
except ImportError:
    print(f"âŒ Groq import failed. Current sys.path: {sys.path}")
    print(f"Checking libs path: {libs_path}")
    if os.path.exists(libs_path):
        print(f"Libs contents: {os.listdir(libs_path)}")
    raise

# Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (API í‚¤ í•„ìš”)
# ì£¼ì˜: ì‹¤ì œ ì‚¬ìš© ì‹œ API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ì„¸ìš”.
client = Groq(api_key="Key")


def get_local_inference(prompt):
    """
    ë¡œì»¬ ì ¯ìŠ¨ì—ì„œ Ollamaë¥¼ í†µí•´ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³  ì§€ì—° ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    ë§¤ë‹ˆì €(LoRAManager)ê°€ ìƒí™©ì— ë§ëŠ” ëª¨ë¸ê³¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    """
    start_time = time.time()

    # ---------------------------------------------------------
    # [Step 1] ìƒí™© ë¶„ì„ (Task ë¶„ë¥˜ - ì„ì‹œë¡œ í‚¤ì›Œë“œ ê¸°ë°˜)
    # ---------------------------------------------------------
    task_type = "general"
    if "code" in prompt or "python" in prompt: task_type = "coding"
    elif "pain" in prompt or "medicine" in prompt: task_type = "medical"
    elif "cook" in prompt or "recipe" in prompt: task_type = "cooking"

    # [Step 2] ë¯¼ê°ë„ ë¶„ì„ (ë³´ì•ˆê´€ í˜¸ì¶œ)
    # (ê°„ë‹¨í•˜ê²Œ í‚¤ì›Œë“œë¡œë§Œ ì²´í¬í•˜ê±°ë‚˜ BERT ê²°ê³¼ í™œìš©)
    is_sensitive = False
    if "password" in prompt or "address" in prompt or "secret" in prompt:
        is_sensitive = True

    # ---------------------------------------------------------
    # [Step 3] ë§¤ë‹ˆì €ì—ê²Œ "ëˆ„ê°€ ë‚˜ê°ˆê¹Œ?" ë¬¼ì–´ë³´ê¸° (í•µì‹¬!)
    # ---------------------------------------------------------
    model_name, sys_prompt, layer_name = manager.select_adapter(task_type, is_sensitive)
    
    print(f"ğŸ¤– [System] {layer_name} ê³„ì¸µ í™œì„±í™” | ì—­í• : {sys_prompt[:30]}...")

    # ---------------------------------------------------------
    # [Step 4] ì‹¤í–‰ (ì„ íƒëœ í˜ë¥´ì†Œë‚˜ ì ìš©)
    # ---------------------------------------------------------
    try:
        response = ollama.chat(model=model_name, messages=[
            {'role': 'system', 'content': sys_prompt}, # <--- ì—¬ê¸°ê°€ ë°”ë€œ!
            {'role': 'user', 'content': prompt}
        ])
        content = response['message']['content']
    except Exception as e:
        print(f"âŒ Error: {e}")
        return "", 0.0

    end_time = time.time()
    latency = end_time - start_time
    
    return content, latency

def get_cloud_inference(prompt):
    """
    Groq APIë¥¼ í†µí•´ í´ë¼ìš°ë“œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³  ì§€ì—° ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    """
    # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3ë²ˆ ì‹œë„)
    for attempt in range(3):
        try:
            start_time = time.time()
            completion = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}]
            )
            end_time = time.time()
            latency = end_time - start_time
            
            # ì„±ê³µí•˜ë©´ ë°”ë¡œ ë¦¬í„´
            return completion.choices[0].message.content, latency

        except Exception as e:
            if "429" in str(e): # 429 ì—ëŸ¬ = "ë„ˆ ë„ˆë¬´ ë§ì´ ì¼ì–´!"
                print(f"âš ï¸ ë¬´ë£Œ í•œë„ ì´ˆê³¼! 10ì´ˆ ì‰½ë‹ˆë‹¤... (ì‹œë„ {attempt+1}/3)")
                time.sleep(10) # 10ì´ˆ íœ´ì‹
            else:
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬: {e}")
                return "", 0.0
    
    return "Cloud Error", 0.0

def get_security_score(prompt, action):
    # 1. ë¡œì»¬(ì ¯ìŠ¨) ì„ íƒ ì‹œ: ë¬´ì¡°ê±´ ì•ˆì „ (1.0ì )
    if action == 0:
        return 1.0
    
    # 2. í´ë¼ìš°ë“œ ì„ íƒ ì‹œ: ë¯¼ê° ì •ë³´ ê²€ì‚¬
    # BERT ëª¨ë¸ì´ ë¬¸ì¥ ë¶„ì„
    results = security_classifier(prompt)
    
    # ê²°ê³¼ í•„í„°ë§ (ì‚¬ëŒ ì´ë¦„, ì¡°ì§, ìœ„ì¹˜ ë“± ê°œì¸ì •ë³´ê°€ ê°ì§€ë˜ë©´)
    sensitive_entities = [res['word'] for res in results if res['score'] > 0.9] # í™•ì‹  90% ì´ìƒë§Œ
    
    if len(sensitive_entities) > 0:
        print(f"ğŸš¨ [Security Alert] ë¯¼ê° ì •ë³´ ê°ì§€ë¨: {sensitive_entities} -> í´ë¼ìš°ë“œ ì „ì†¡ ì°¨ë‹¨!")
        return 0.0 # ë³´ì•ˆ ìœ„ë°˜! (ì ìˆ˜ ê¹ìŒ)
    
    return 1.0 # ì•ˆì „í•¨
        
# --- í…ŒìŠ¤íŠ¸ ì½”ë“œ (ì§ì ‘ ì‹¤í–‰ ì‹œì—ë§Œ ì‘ë™) ---
if __name__ == "__main__":
    test_prompt = "AIê°€ ë¯¸ë˜ì— ì¸ë¥˜ì—ê²Œ ë¯¸ì¹  ì˜í–¥ì€?"
    print(f"ì§ˆë¬¸: {test_prompt}")
    print("â³ ìƒê° ì¤‘...")
    
    answer, lat = get_local_inference(test_prompt)
    
    print("-" * 30)
    print(f"ğŸ’¡ ë‹µë³€: {answer[:100]}...") # ë„ˆë¬´ ê¸°ë‹ˆê¹Œ ì•ë¶€ë¶„ë§Œ ì¶œë ¥
    print("-" * 30)
    print(f"â±ï¸ ì§€ì—° ì‹œê°„(Latency): {lat:.4f}ì´ˆ")
    print("âœ… TMO ì‹œìŠ¤í…œì— ì‚¬ìš©í•  ì¤€ë¹„ ì™„ë£Œ!")
